import 'package:dio/dio.dart';
import 'package:html/parser.dart' as html_parser;
import 'package:html/dom.dart' as dom;
import '../../models/story_model.dart';
import '../../models/chapter_model.dart';

class SyosetuScraper {
  static final SyosetuScraper _instance = SyosetuScraper._internal();
  factory SyosetuScraper() => _instance;
  SyosetuScraper._internal();

  // Scrape story t·ª´ Syosetu s·ª≠ d·ª•ng Dio (nhanh v√† ·ªïn ƒë·ªãnh)
  Future<Story?> scrapeStoryWithWebView(String url) async {
    try {
      print('üöÄ Starting Syosetu story scraping with Dio for: $url');

      final dio = _createDioInstance();
      final response = await dio.get(url);

      if (response.statusCode != 200) {
        print('‚ùå HTTP ${response.statusCode} for: $url');
        return null;
      }

      final document = html_parser.parse(response.data);
      final story = await _extractStoryFromDocument(document, url);

      if (story != null) {
        print('‚úÖ Successfully scraped Syosetu story: ${story.title}');
      } else {
        print('‚ùå Failed to extract story information');
      }

      return story;
    } catch (e, stackTrace) {
      print('‚ùå Error in Syosetu Dio story scraping: $e');
      print('Stack trace: $stackTrace');
      return null;
    }
  }

  // Scrape t·ª´ Syosetu v·ªõi chapters (s·ª≠ d·ª•ng Dio - nhanh v√† ·ªïn ƒë·ªãnh)
  Future<Map<String, dynamic>?> scrapeStoryWithChaptersWebView(String url, {bool scrapeContent = false}) async {
    // Chuy·ªÉn sang s·ª≠ d·ª•ng Dio thay v√¨ WebView
    return await scrapeStoryWithChaptersDio(url, scrapeContent: scrapeContent);
  }



  // Tr√≠ch xu·∫•t th√¥ng tin story t·ª´ document
  Future<Story?> _extractStoryFromDocument(dom.Document document, String url) async {
    try {
      print('Extracting Syosetu story information from rendered HTML...');
      print('Document title: ${document.querySelector('title')?.text ?? "No title"}');
      
      // Tr√≠ch xu·∫•t th√¥ng tin t·ª´ HTML
      final title = _extractTitle(document);
      print('Extracted title: "$title"');
      
      final author = _extractAuthor(document);
      print('Extracted author: "$author"');
      
      final description = _extractDescription(document);
      print('Extracted description length: ${description.length}');
      
      // Syosetu th∆∞·ªùng kh√¥ng c√≥ ·∫£nh b√¨a
      final coverImageUrl = '';
      
      // Syosetu kh√¥ng c√≥ th·ªÉ lo·∫°i r√µ r√†ng nh∆∞ Hako
      final genres = <String>['Light Novel'];
      
      // Syosetu kh√¥ng c√≥ tr·∫°ng th√°i r√µ r√†ng
      final status = 'ƒêang ti·∫øn h√†nh';

      if (title.isEmpty) {
        print('Title is empty, returning null');
        return null;
      }

      // T·∫°o ID unique t·ª´ URL
      final storyId = _generateStoryId(url);
      print('Generated story ID: $storyId');

      final story = Story(
        id: storyId,
        title: title,
        author: author,
        description: description,
        coverImageUrl: coverImageUrl,
        sourceUrl: url,
        sourceWebsite: 'Syosetu',
        genres: genres,
        status: status,
        translator: '', // Syosetu l√† ti·∫øng Nh·∫≠t g·ªëc
        originalLanguage: 'Nh·∫≠t B·∫£n',
        metadata: {
          'scraped_at': DateTime.now().toIso8601String(),
          'scraper_version': '3.0_webview_syosetu',
        },
      );
      
      print('Syosetu story created successfully: ${story.title}');
      return story;
    } catch (e, stackTrace) {
      print('Error extracting Syosetu story from document: $e');
      print('Stack trace: $stackTrace');
      return null;
    }
  }

  // Tr√≠ch xu·∫•t th√¥ng tin story v·ªõi chapters t·ª´ document
  Future<Map<String, dynamic>?> _extractStoryWithChaptersFromDocument(dom.Document document, String url, {bool scrapeContent = false}) async {
    try {
      print('Extracting Syosetu story with chapters from rendered HTML...');

      // Tr√≠ch xu·∫•t th√¥ng tin story c∆° b·∫£n
      final story = await _extractStoryFromDocument(document, url);
      if (story == null) {
        print('Failed to extract Syosetu story information');
        return null;
      }

      // Tr√≠ch xu·∫•t danh s√°ch chapters
      final chapters = await _extractChaptersFromDocument(document, story.id, url);
      print('Extracted ${chapters.length} Syosetu chapters');

      // C·∫≠p nh·∫≠t totalChapters cho story
      final updatedStory = story.copyWith(totalChapters: chapters.length);

      // Scrape n·ªôi dung chapters n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu
      if (scrapeContent && chapters.isNotEmpty) {
        print('Starting to scrape Syosetu chapter contents...');
        await _scrapeChapterContents(chapters);
      }

      return {
        'story': updatedStory,
        'chapters': chapters,
        'scraped_content': scrapeContent,
      };
    } catch (e, stackTrace) {
      print('Error extracting Syosetu story with chapters: $e');
      print('Stack trace: $stackTrace');
      return null;
    }
  }

  // Ki·ªÉm tra xem URL c√≥ ph·∫£i Syosetu kh√¥ng
  bool canScrapeUrl(String url) {
    return url.contains('syosetu.com') || url.contains('ncode.syosetu.com');
  }

  // L·∫•y t√™n website t·ª´ URL
  String getWebsiteName(String url) {
    if (url.contains('syosetu.com') || url.contains('ncode.syosetu.com')) return 'Syosetu';
    return 'Unknown';
  }

  // Tr√≠ch xu·∫•t danh s√°ch chapters t·ª´ document v·ªõi h·ªó tr·ª£ ph√¢n trang
  Future<List<Chapter>> _extractChaptersFromDocument(dom.Document document, String storyId, String baseUrl) async {
    final chapters = <Chapter>[];

    try {
      // Tr√≠ch xu·∫•t chapters t·ª´ trang hi·ªán t·∫°i
      await _extractChaptersFromCurrentPage(document, chapters, storyId, baseUrl);

      // Ki·ªÉm tra c√≥ ph√¢n trang kh√¥ng
      final nextPageUrl = _getNextPageUrl(document, baseUrl);
      if (nextPageUrl != null) {
        print('üîÑ FOUND PAGINATION! This story has multiple pages');
        print('üìñ Chapters from page 1: ${chapters.length}');
        print('üöÄ Starting to scrape additional pages to get ALL chapters...');
        await _scrapeAdditionalPages(nextPageUrl, chapters, storyId, baseUrl);
        print('‚úÖ PAGINATION COMPLETE! Total chapters: ${chapters.length}');
      } else {
        print('üìÑ Single page story, no pagination needed');
      }

      print('Successfully extracted ${chapters.length} total Syosetu chapters');
      return chapters;
    } catch (e) {
      print('Error extracting Syosetu chapters: $e');
      return chapters;
    }
  }

  // Tr√≠ch xu·∫•t chapters t·ª´ trang hi·ªán t·∫°i
  Future<void> _extractChaptersFromCurrentPage(dom.Document document, List<Chapter> chapters, String storyId, String baseUrl) async {
    // T√¨m t·∫•t c·∫£ chapter links trong .p-eplist
    final chapterElements = document.querySelectorAll('.p-eplist .p-eplist__sublist');
    print('Found ${chapterElements.length} Syosetu chapters on current page');

    for (final chapterElement in chapterElements) {
      try {
        final chapterLink = chapterElement.querySelector('a.p-eplist__subtitle');
        if (chapterLink == null) continue;

        final chapterTitle = chapterLink.text.trim();
        final chapterUrl = chapterLink.attributes['href'] ?? '';

        if (chapterTitle.isEmpty || chapterUrl.isEmpty) continue;

        // T·∫°o URL ƒë·∫ßy ƒë·ªß
        final fullChapterUrl = chapterUrl.startsWith('http')
            ? chapterUrl
            : '${Uri.parse(baseUrl).origin}$chapterUrl';

        // Tr√≠ch xu·∫•t th·ªùi gian publish t·ª´ .p-eplist__update
        final updateElement = chapterElement.querySelector('.p-eplist__update');
        final updateText = updateElement?.text.trim() ?? '';
        final publishedAt = _parseDate(updateText);

        // T·∫°o chapter ID t·ª´ URL
        final chapterId = _generateChapterId(fullChapterUrl);

        final chapter = Chapter(
          id: chapterId,
          storyId: storyId,
          title: chapterTitle,
          url: fullChapterUrl,
          chapterNumber: chapters.length + 1, // ƒê√°nh s·ªë li√™n t·ª•c
          volumeTitle: '', // Syosetu kh√¥ng c√≥ volume
          volumeNumber: 0,
          publishedAt: publishedAt,
          hasImages: false, // Syosetu √≠t khi c√≥ ·∫£nh
          metadata: {
            'scraped_at': DateTime.now().toIso8601String(),
            'scraper_version': '3.0_webview_syosetu_pagination',
            'update_info': updateText,
          },
        );

        chapters.add(chapter);
        print('Added Syosetu chapter: $chapterTitle');
      } catch (e) {
        print('Error processing Syosetu chapter: $e');
      }
    }
  }

  // L·∫•y URL trang ti·∫øp theo
  String? _getNextPageUrl(dom.Document document, String baseUrl) {
    final nextLink = document.querySelector('.c-pager__item--next[href]');
    if (nextLink != null) {
      final href = nextLink.attributes['href'];
      if (href != null && href.isNotEmpty) {
        final nextUrl = href.startsWith('http')
            ? href
            : '${Uri.parse(baseUrl).origin}$href';
        print('Found next page URL: $nextUrl');
        return nextUrl;
      }
    }
    return null;
  }

  // Scrape c√°c trang b·ªï sung (ch·ªâ d√πng Dio)
  Future<void> _scrapeAdditionalPages(String nextPageUrl, List<Chapter> chapters, String storyId, String baseUrl) async {
    // Chuy·ªÉn sang d√πng Dio ngay t·ª´ ƒë·∫ßu
    print('üöÄ Starting Dio-based pagination scraping from page 2...');
    await _scrapePaginationWithDio(baseUrl, chapters, storyId, 2);
  }

  // Scrape pagination s·ª≠ d·ª•ng Dio (nhanh h∆°n WebView)
  Future<void> _scrapePaginationWithDio(String baseUrl, List<Chapter> chapters, String storyId, int startPage) async {
    print('üöÄ Starting Dio-based pagination scraping from page $startPage...');

    final dio = _createDioInstance();

    int pageNum = startPage;
    const maxPages = 50;

    while (pageNum <= maxPages) {
      try {
        final baseUri = Uri.parse(baseUrl);
        final pageUrl = '${baseUri.origin}${baseUri.path}?p=$pageNum';
        print('üìÑ Scraping page $pageNum with Dio: $pageUrl');

        final response = await dio.get(pageUrl);
        if (response.statusCode == 200) {
          final document = html_parser.parse(response.data);

          // Ki·ªÉm tra c√≥ chapters kh√¥ng
          final chapterElements = document.querySelectorAll('.p-eplist .p-eplist__sublist');
          if (chapterElements.isEmpty) {
            print('No chapters found on page $pageNum, pagination complete');
            break;
          }

          final initialCount = chapters.length;
          await _extractChaptersFromCurrentPage(document, chapters, storyId, baseUrl);
          final newChapters = chapters.length - initialCount;
          print('‚úÖ Added $newChapters chapters from page $pageNum (Total: ${chapters.length})');

          // Ki·ªÉm tra c√≥ trang ti·∫øp theo kh√¥ng
          final nextPageLink = document.querySelector('.c-pager__item--next[href]');
          if (nextPageLink == null) {
            print('No next page link found, pagination complete');
            break;
          }

          pageNum++;

          // Delay ng·∫Øn ƒë·ªÉ tr√°nh spam
          await Future.delayed(const Duration(milliseconds: 500));
        } else {
          print('HTTP ${response.statusCode} for page $pageNum, stopping');
          break;
        }
      } catch (e) {
        print('Error scraping page $pageNum with Dio: $e');
        // Th·ª≠ ti·∫øp trang sau
        pageNum++;
        if (pageNum > startPage + 5) {
          // N·∫øu fail qu√° 5 trang li√™n ti·∫øp th√¨ d·ª´ng
          print('Too many consecutive failures, stopping Dio pagination');
          break;
        }
      }
    }

    print('üéâ Dio pagination completed! Total chapters: ${chapters.length}');
  }

  // T·∫°o Dio instance v·ªõi headers chu·∫©n
  Dio _createDioInstance() {
    final dio = Dio();
    dio.options.headers = {
      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
      'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
      'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
      'Accept-Encoding': 'gzip, deflate',
      'Connection': 'keep-alive',
      'Upgrade-Insecure-Requests': '1',
    };
    return dio;
  }

  // Scrape t·ª´ Syosetu v·ªõi chapters s·ª≠ d·ª•ng Dio (nhanh v√† ·ªïn ƒë·ªãnh)
  Future<Map<String, dynamic>?> scrapeStoryWithChaptersDio(String url, {bool scrapeContent = false}) async {
    try {
      print('üöÄ Starting Syosetu scraping with Dio for: $url');

      final dio = _createDioInstance();

      // Scrape trang ƒë·∫ßu ti√™n
      print('üìÑ Fetching page 1...');
      final response = await dio.get(url);
      if (response.statusCode != 200) {
        print('‚ùå HTTP ${response.statusCode} for main page');
        return null;
      }

      final document = html_parser.parse(response.data);
      print('‚úÖ Page 1 parsed successfully');

      // Tr√≠ch xu·∫•t story info v√† chapters
      final result = await _extractStoryWithChaptersFromDocument(document, url, scrapeContent: scrapeContent);
      if (result == null) {
        print('‚ùå Failed to extract story information');
        return null;
      }

      print('üéâ Syosetu scraping completed successfully!');
      return result;

    } catch (e, stackTrace) {
      print('‚ùå Error in Syosetu Dio scraping: $e');
      print('Stack trace: $stackTrace');
      return null;
    }
  }



  // Tr√≠ch xu·∫•t ti√™u ƒë·ªÅ t·ª´ Syosetu
  String _extractTitle(dom.Document document) {
    print('DEBUG: Looking for Syosetu title...');

    // Debug: T√¨m t·∫•t c·∫£ elements c√≥ class ch·ª©a "novel"
    final novelElements = document.querySelectorAll('[class*="novel"]');
    print('DEBUG: Found ${novelElements.length} elements with "novel" in class');
    for (int i = 0; i < novelElements.length && i < 5; i++) {
      final element = novelElements[i];
      print('DEBUG: Novel element $i: ${element.className} - ${element.text.trim().substring(0, element.text.trim().length.clamp(0, 50))}');
    }

    // T√¨m ti√™u ƒë·ªÅ trong .p-novel__title
    final titleElement = document.querySelector('.p-novel__title');
    print('DEBUG: .p-novel__title found: ${titleElement != null}');
    if (titleElement != null) {
      final title = titleElement.text.trim();
      print('DEBUG: Title from .p-novel__title: "$title"');
      if (title.isNotEmpty) {
        return title;
      }
    }

    // Th·ª≠ t√¨m v·ªõi h1.p-novel__title
    final h1TitleElement = document.querySelector('h1.p-novel__title');
    print('DEBUG: h1.p-novel__title found: ${h1TitleElement != null}');
    if (h1TitleElement != null) {
      final title = h1TitleElement.text.trim();
      print('DEBUG: Title from h1.p-novel__title: "$title"');
      if (title.isNotEmpty) {
        return title;
      }
    }

    // Th·ª≠ l·∫•y t·ª´ title tag
    final titleTag = document.querySelector('title');
    print('DEBUG: title tag found: ${titleTag != null}');
    if (titleTag != null) {
      final titleText = titleTag.text.trim();
      print('DEBUG: Title tag content: "$titleText"');
      // Lo·∫°i b·ªè ph·∫ßn " - Â∞èË™¨ÂÆ∂„Å´„Å™„Çç„ÅÜ" ·ªü cu·ªëi
      final cleanTitle = titleText.replaceAll(
        RegExp(r' - Â∞èË™¨ÂÆ∂„Å´„Å™„Çç„ÅÜ.*$'),
        '',
      );
      if (cleanTitle.isNotEmpty) {
        return cleanTitle;
      }
    }

    print('DEBUG: No Syosetu title found');
    return '';
  }

  // Tr√≠ch xu·∫•t t√°c gi·∫£ t·ª´ Syosetu
  String _extractAuthor(dom.Document document) {
    // T√¨m t√°c gi·∫£ trong .p-novel__author
    final authorElement = document.querySelector('.p-novel__author a');
    if (authorElement != null) {
      return authorElement.text.trim();
    }

    // Th·ª≠ t√¨m trong text c·ªßa .p-novel__author
    final authorDiv = document.querySelector('.p-novel__author');
    if (authorDiv != null) {
      final authorText = authorDiv.text.trim();
      // Lo·∫°i b·ªè "‰ΩúËÄÖÔºö" ·ªü ƒë·∫ßu
      final cleanAuthor = authorText.replaceAll(RegExp(r'^‰ΩúËÄÖÔºö'), '');
      if (cleanAuthor.isNotEmpty) {
        return cleanAuthor;
      }
    }

    return 'Kh√¥ng r√µ';
  }

  // Tr√≠ch xu·∫•t m√¥ t·∫£ t·ª´ Syosetu
  String _extractDescription(dom.Document document) {
    // T√¨m ph·∫ßn t√≥m t·∫Øt trong #novel_ex ho·∫∑c .p-novel__summary
    final summaryElement = document.querySelector('#novel_ex') ??
                          document.querySelector('.p-novel__summary');

    if (summaryElement != null) {
      // L·∫•y text v√† x·ª≠ l√Ω c√°c th·∫ª <br>
      final htmlContent = summaryElement.innerHtml;
      final cleanContent = htmlContent
          .replaceAll('<br>', '\n')
          .replaceAll('<br/>', '\n')
          .replaceAll('<br />', '\n');

      // Parse l·∫°i ƒë·ªÉ l·∫•y text thu·∫ßn
      final tempDoc = html_parser.parseFragment(cleanContent);
      final description = tempDoc.text?.trim() ?? '';

      if (description.isNotEmpty) {
        return description;
      }
    }

    return '';
  }

  // Scrape n·ªôi dung c·ªßa c√°c chapters
  Future<void> _scrapeChapterContents(List<Chapter> chapters) async {
    for (int i = 0; i < chapters.length; i++) {
      final chapter = chapters[i];
      print('Scraping Syosetu content for chapter ${i + 1}/${chapters.length}: ${chapter.title}');

      try {
        final content = await scrapeChapterContent(chapter.url);
        if (content.isNotEmpty) {
          chapters[i] = chapter.copyWith(
            content: content,
            wordCount: _countWords(content),
          );
          print('Successfully scraped Syosetu content for: ${chapter.title}');
        } else {
          print('No Syosetu content found for: ${chapter.title}');
        }
      } catch (e) {
        print('Error scraping Syosetu content for ${chapter.title}: $e');
      }

      // Delay ƒë·ªÉ tr√°nh spam requests
      await Future.delayed(const Duration(milliseconds: 500));
    }
  }

  // Scrape n·ªôi dung c·ªßa m·ªôt chapter s·ª≠ d·ª•ng Dio
  Future<String> scrapeChapterContent(String chapterUrl) async {
    try {
      print('üìÑ Scraping chapter content with Dio: $chapterUrl');
      final dio = _createDioInstance();

      final response = await dio.get(chapterUrl);
      if (response.statusCode != 200) {
        print('‚ùå HTTP ${response.statusCode} for chapter: $chapterUrl');
        return '';
      }

      final document = html_parser.parse(response.data);
      final content = _extractChapterContentFromDocument(document);

      if (content.isNotEmpty) {
        print('‚úÖ Successfully scraped chapter content (${content.length} chars)');
      } else {
        print('‚ö†Ô∏è No content found for chapter');
      }

      return content;
    } catch (e) {
      print('‚ùå Error scraping Syosetu chapter content: $e');
      return '';
    }
  }

  // Tr√≠ch xu·∫•t n·ªôi dung chapter t·ª´ document
  String _extractChapterContentFromDocument(dom.Document document) {
    try {
      // T√¨m content container cho Syosetu - c·∫•u tr√∫c m·ªõi
      final contentElement = document.querySelector('.p-novel__text') ??
                            document.querySelector('#novel_honbun') ??
                            document.querySelector('.novel_view') ??
                            document.querySelector('#honbun');

      if (contentElement != null) {
        // L·∫•y t·∫•t c·∫£ th·∫ª p v√† x·ª≠ l√Ω
        final paragraphs = contentElement.querySelectorAll('p');
        final contentLines = <String>[];

        for (final p in paragraphs) {
          final text = p.text.trim();
          if (text.isNotEmpty) {
            contentLines.add(text);
          } else {
            // Th√™m d√≤ng tr·ªëng cho <br> ho·∫∑c <p> r·ªóng
            contentLines.add('');
          }
        }

        final content = contentLines.join('\n');
        print('üìñ Extracted chapter content: ${content.length} chars, ${paragraphs.length} paragraphs');
        return content;
      }

      // Fallback: l·∫•y to√†n b·ªô text trong .p-novel__body
      final bodyElement = document.querySelector('.p-novel__body');
      if (bodyElement != null) {
        final content = bodyElement.text.trim();
        print('üìñ Fallback extracted content: ${content.length} chars');
        return content;
      }

      print('‚ùå No content container found');
      return '';
    } catch (e) {
      print('‚ùå Error extracting chapter content: $e');
      return '';
    }
  }

  // Utility methods
  DateTime _parseDate(String dateText) {
    try {
      // X·ª≠ l√Ω format date c·ªßa Syosetu: "2025/06/08 19:39"
      if (dateText.contains('/') && dateText.contains(' ')) {
        final parts = dateText.split(' ');
        if (parts.length >= 2) {
          final datePart = parts[0];
          final timePart = parts[1];

          final dateComponents = datePart.split('/');
          final timeComponents = timePart.split(':');

          if (dateComponents.length == 3 && timeComponents.length >= 2) {
            final year = int.parse(dateComponents[0]);
            final month = int.parse(dateComponents[1]);
            final day = int.parse(dateComponents[2]);
            final hour = int.parse(timeComponents[0]);
            final minute = int.parse(timeComponents[1]);

            return DateTime(year, month, day, hour, minute);
          }
        }
      }

      // Fallback
      return DateTime.now();
    } catch (e) {
      return DateTime.now();
    }
  }

  String _generateStoryId(String url) {
    final uri = Uri.parse(url);
    final pathSegments = uri.pathSegments;

    // L·∫•y ID t·ª´ URL Syosetu (v√≠ d·ª•: n1706ko)
    for (final segment in pathSegments) {
      if (RegExp(r'^n\d+[a-z]+$').hasMatch(segment)) {
        return '${uri.host}_$segment';
      }
    }

    // Fallback: s·ª≠ d·ª•ng hash c·ªßa URL
    return '${uri.host}_${url.hashCode.abs()}';
  }

  String _generateChapterId(String url) {
    final uri = Uri.parse(url);
    final pathSegments = uri.pathSegments;

    // T√¨m segment ch·ª©a chapter ID (v√≠ d·ª•: n1706ko/1/)
    if (pathSegments.length >= 2) {
      final storyId = pathSegments[0];
      final chapterNum = pathSegments[1];
      if (RegExp(r'^n\d+[a-z]+$').hasMatch(storyId) && RegExp(r'^\d+$').hasMatch(chapterNum)) {
        return '${uri.host}_${storyId}_c$chapterNum';
      }
    }

    // Fallback: s·ª≠ d·ª•ng hash c·ªßa URL
    return '${uri.host}_chapter_${url.hashCode.abs()}';
  }

  int _countWords(String text) {
    if (text.isEmpty) return 0;
    // ƒê·∫øm k√Ω t·ª± cho ti·∫øng Nh·∫≠t thay v√¨ t·ª´
    return text.trim().length;
  }
}
